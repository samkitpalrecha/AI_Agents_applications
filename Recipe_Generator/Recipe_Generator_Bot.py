from langchain_groq import ChatGroq
from dotenv import load_dotenv
import os

load_dotenv()

# Load the LLaMA model
llama_model = ChatGroq(temperature = 0.3, groq_api_key=os.getenv("GROQ_API_KEY"))

from transformers import AutoTokenizer, AutoModelForCausalLM
from langchain.prompts import PromptTemplate
import torch
from torch.quantization import quantize_dynamic
import time

# 1. Load Tokenizer
model_name = "mbien/recipenlg"
tokenizer = AutoTokenizer.from_pretrained(model_name)
tokenizer.pad_token = tokenizer.eos_token

# 2. Load and Quantize Model
print("Loading and quantizing model...")
model = AutoModelForCausalLM.from_pretrained(model_name)

# model = quantize_dynamic(
#     model,
#     {torch.nn.Linear},  # Target layers for quantization
#     dtype=torch.qint8  # Use 16-bit floats for weights
# )

device = "cuda" if torch.cuda.is_available() else "cpu"
model.to(device)

print("Model quantization completed.")

# 3. LangChain Prompt Template for Structured Output
template_nlg = """
You are an expert recipe generator. Generate the sensible recipe for the following:
{query}
"""

prompt_nlg = PromptTemplate(template=template_nlg, input_variables=["query"])

# 4. Optimized Generation Function
def generate_recipe(query, max_length=400):
    structured_prompt = prompt_nlg.format(query=query)
    input_ids = tokenizer.encode(structured_prompt, return_tensors="pt").to(device)
    
    # Timing for performance analysis
    start_time = time.time()
    
    attention_mask = torch.ones(input_ids.shape, device=device)
    outputs = model.generate(
        input_ids,
        attention_mask=attention_mask,
        max_length=max_length,
        temperature=0.4,
        top_k=10,
        top_p=0.8,
        do_sample=True,
        num_return_sequences=1,
        repetition_penalty=1.5
    )
    
    end_time = time.time()
    print(f"Time taken for generation: {end_time - start_time:.2f} seconds")
    
    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return generated_text

# 5. Beautify Output
from textwrap import fill

def print_beautifully(text):
    formatted_text = fill(text, width=100)
    print("\n" + "="*40 + " DRAFT RECIPE " + "="*40 + "\n")
    print(formatted_text)
    print("\n" + "="*40 + " FINAL RECIPE " + "="*40 + "\n")

# 6. Generate and Display Recipe
query = input("Give the name of a dish to get the recipe of:")
if "recipe" or "Recipe" not in query:
    query = "Recipe of " + query
structured_prompt = prompt_nlg.format(query=query)

generated_recipe = generate_recipe(query)

# print_beautifully(generated_recipe)

from langchain_core.output_parsers import JsonOutputParser, StrOutputParser
from langchain.output_parsers import StructuredOutputParser

# Define a prompt template that includes the context (recipe) and the query
prompt_template = """
You are a recipe manager. You are given a query and a recipe based on that query, generated by your assistent.
If the recipe generated by your assistent is wrong, just correct it according to the given query.
So, Your task is to provide an improved and corrected response to the query based on the recipe with the following formatting:
Title: [Recipe Name] Recipe
Ingredients:
   - [Ingredient 1]
   - [Ingredient 2]
Instructions:
   1. [Step 1]
   2. [Step 2]
Keep the output clean, clear and focused on the recipe, do not include anything else.
Recipe: {recipe}
Query: {query}
Answer:
"""

prompt_llama = PromptTemplate(
    input_variables=["recipe", "query"],
    template=prompt_template
)

parser = StrOutputParser()

# Define the chain with Llama model and the prompt template
chain = prompt_llama | llama_model | parser

recipe_string = chain.invoke({"recipe": generated_recipe, "query": query})

print(recipe_string)